# ğŸ“Š Directorio `data/` - Datasets

## DescripciÃ³n

Este directorio contiene los **datasets** utilizados para entrenamiento, validaciÃ³n y prueba del modelo de predicciÃ³n de churn.

## Estructura

```
data/
â”œâ”€â”€ README.md                    # Este archivo
â”œâ”€â”€ dataset.csv                  # Dataset principal de entrenamiento
â”œâ”€â”€ dataset_train.csv           # Datos de entrenamiento (70%)
â”œâ”€â”€ dataset_test.csv            # Datos de prueba (30%)
â””â”€â”€ dataset_synthetic.csv       # Datos sintÃ©ticos generados
```

## Datasets Disponibles

### `dataset.csv` (Principal)

- **PropÃ³sito:** Dataset completo para entrenamiento
- **Filas:** 7000+ registros
- **Columnas:** 20+ caracterÃ­sticas
- **GeneraciÃ³n:** Script `train_model.py` genera automÃ¡ticamente si no existe
- **Formato:** CSV con headers

### `dataset_train.csv`

- **PropÃ³sito:** Subconjunto para entrenamiento del modelo
- **ProporciÃ³n:** 70% del dataset principal
- **Filas:** ~4900 registros
- **Uso:** Entrenar RandomForestClassifier

### `dataset_test.csv`

- **PropÃ³sito:** Subconjunto para validaciÃ³n
- **ProporciÃ³n:** 30% del dataset principal
- **Filas:** ~2100 registros
- **Uso:** Evaluar rendimiento del modelo

### `dataset_synthetic.csv`

- **PropÃ³sito:** Datos sintÃ©ticos generados para testing
- **Filas:** 500+ registros
- **GeneraciÃ³n:** Script `scripts/generate_synthetic_data.py`
- **Uso:** Pruebas sin datos reales

## CaracterÃ­sticas del Dataset

```json
{
  "Customer_ID": "Identificador Ãºnico",
  "Age": "Edad del cliente (18-70 aÃ±os)",
  "Location": "UbicaciÃ³n (Urban, Suburban, Rural)",
  "Income_Level": "Nivel de ingresos (Low, Medium, High)",
  "Total_Transactions": "NÃºmero total de transacciones (1-1000)",
  "Avg_Transaction_Value": "Valor promedio de transacciones ($10-$1000)",
  "Total_Spent": "Total gastado (suma de transacciones)",
  "Active_Days": "DÃ­as activos en plataforma (1-365)",
  "Last_Transaction_Days_Ago": "DÃ­as desde Ãºltima transacciÃ³n (1-365)",
  "App_Usage_Frequency": "Frecuencia de uso (Daily, Weekly, Monthly)",
  "Customer_Satisfaction_Score": "SatisfacciÃ³n del cliente (1-10)",
  "Churn": "Variable target - Abandono (0=No, 1=SÃ­)"
}
```

## CÃ³mo Usar los Datos

### 1. Generar Dataset AutomÃ¡ticamente

```bash
cd data-science
python scripts/train_model.py
# Genera dataset.csv automÃ¡ticamente si no existe
```

### 2. Dividir en Train/Test

```python
from sklearn.model_selection import train_test_split
import pandas as pd

df = pd.read_csv('data/dataset.csv')
train, test = train_test_split(df, test_size=0.3, random_state=42)
train.to_csv('data/dataset_train.csv', index=False)
test.to_csv('data/dataset_test.csv', index=False)
```

### 3. Cargar en Scripts

```python
import pandas as pd
df = pd.read_csv('data/dataset.csv')
print(f"Loaded {len(df)} records")
```

## EstadÃ­sticas del Dataset

- **Total de registros:** ~7000
- **CaracterÃ­sticas numÃ©ricas:** 13
- **CaracterÃ­sticas categÃ³ricas:** 7
- **Variable target (Churn):** Binaria (0/1)
- **Desbalance de clases:** ~30% Churn, ~70% No Churn
- **Sin datos faltantes:** âœ…

## ğŸ” Privacidad

âš ï¸ **Para ProducciÃ³n:**

- Usar datos reales SOLO con consentimiento
- Anonimizar IDs de clientes
- Cumplir con GDPR/CCPA
- Datos sintÃ©ticos para desarrollo/testing

## ğŸ“ Notas

- Los datos se generan sintÃ©ticamente con `np.random.seed(42)` para reproducibilidad
- Para datos reales, reemplazar `dataset.csv` manteniendo estructura
- Validar que features coincidan con `config.py`

---

**Ãšltima actualizaciÃ³n:** Diciembre 2025
